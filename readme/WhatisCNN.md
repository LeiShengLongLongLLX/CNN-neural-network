# CNN(Covlution Neural Network)

## 一、CNN总概

### 1.1 CNN是什么？

*   **定义**：卷积神经网络是一种专为处理**网格状数据**（如图像、视频、音频）而设计的深度学习模型。
*   **核心思想**：通过**卷积** 操作来自动、高效地学习数据的空间层次特征。
*   **与传统神经网络的对比**：
    *   **传统神经网络（如全连接网络）**：将输入数据（如图像）展平为一维向量，会完全丢失空间信息，且参数数量巨大，容易过拟合。
    *   **CNN**：保留了数据的空间结构，通过**局部连接**和**权值共享** 大大减少了参数数量，使其更高效、更易于训练。

### 1.2 为什么CNN特别适合图像处理？

1.  **局部相关性**：图像中一个像素与其周围像素的关系最紧密，与遥远像素的关系较弱。CNN的卷积操作正是关注局部区域。
2.  **平移不变性**：无论一只猫在图像的左上角还是右下角，它都是一只猫。CNN通过池化操作和层级结构，使得网络对目标的位置变化不敏感。
3.  **尺度不变性**：通过多层卷积和池化，CNN可以从底层边缘、纹理，到中层部分器官，再到高层整体物体，逐步构建出对图像的尺度鲁棒性理解。

---

### 1.3 CNN的核心组件

一个典型的CNN由以下几部分组成：

#### 1.3.1 卷积层 - 特征提取的核心

*   **目的**：使用**滤波器（或称为卷积核）** 在输入数据上滑动，提取局部特征（如边缘、角点、颜色块）。
*   **关键概念**：
    *   **滤波器**：一个小尺寸的权重矩阵（如3x3, 5x5）。不同的滤波器用于提取不同的特征。
    *   **感受野**：滤波器在输入图像上每次覆盖的区域大小。
    *   **步长**：滤波器每次移动的像素数。步长越大，输出特征图尺寸越小。
    *   **填充**：在输入图像边缘填充一圈像素（通常用0填充）。目的是为了控制输出特征图的尺寸。
    *   **深度**：一个卷积层通常使用多个滤波器，每个滤波器会产生一个**特征图**。所有这些特征图堆叠起来，就构成了该卷积层的输出。
*   **工作机制**：滤波器在输入上滑动，在每个位置进行**点乘** 求和，再加上一个偏置项，最终生成特征图。

#### 1.3.2 激活函数 - 引入非线性

*   **目的**：为网络引入非线性因素，使其能够学习并模拟复杂的非线性关系。没有它，多层网络就等价于一个单层线性模型。
*   **常用函数**：
    *   **ReLU（修正线性单元）**：`f(x) = max(0, x)`。目前最常用，因为它能有效缓解梯度消失问题，且计算简单。
    *   **Sigmoid / Tanh**：在早期使用，现在多用于输出层（如二分类）。

#### 1.3.3 池化层 - 降维和保持平移不变性

*   **目的**：对特征图进行**下采样**，减少数据尺寸和参数量，防止过拟合，同时扩大后续卷积层的感受野，并赋予网络一定的平移不变性。
*   **特点**：没有需要学习的参数。
*   **常用方法**：
    *   **最大池化**：取池化窗口内的最大值。效果最好，最常用。
    *   **平均池化**：取池化窗口内的平均值。
*   **工作机制**：类似卷积，有一个窗口和步长，在特征图上滑动，但执行的是最大或平均操作。

#### 1.3.4. 全连接层 - 最终分类

*   **目的**：位于网络的末端，将前面学习到的所有分布式特征映射到最终的样本标签空间（进行分类或回归）。
*   **工作机制**：将最后一层卷积或池化输出的多维特征图**展平** 成一个一维向量，然后像传统神经网络一样进行连接。通常最后会接一个**Softmax** 函数，输出每个类别的概率。

**总结表**

| 组件 | 主要功能 | 关键优势 |
| :--- | :--- | :--- |
| **卷积层** | 提取局部空间特征 | 权值共享、局部连接，大幅减少参数 |
| **激活函数** | 引入非线性 | 使网络能拟合复杂函数（ReLU最常用） |
| **池化层** | 下采样，压缩特征图 | 降维、防止过拟合、提供平移不变性 |
| **全连接层** | 将特征映射到样本标签 | 完成最终的分类或回归任务 |

---

### 1.4 经典的CNN网络架构

*   **LeNet-5 (1998)**：CNN的开山之作，用于手写数字识别，结构为：`卷积 -> 池化 -> 卷积 -> 池化 -> 全连接 -> 输出`。
*   **AlexNet (2012)**：在ImageNet大赛中一战成名，真正开启了深度学习热潮。它更深，使用了ReLU、Dropout等技术。
*   **VGGNet (2014)**：探索了网络深度，通过堆叠小的3x3卷积核来替代大的卷积核，结构非常规整。
*   **GoogLeNet (2014)**：引入了**Inception模块**，在增加网络深度和宽度的同时，减少了参数量。
*   **ResNet (2015)**：提出了**残差块** 和**跳跃连接**，解决了极深网络的梯度消失和退化问题，使得构建上百甚至上千层的网络成为可能。
*   **MobileNet (2017)**：引入了**深度可分离卷积**为核心构建块，专为移动和嵌入式设备等资源受限环境设计，在速度和体积上实现了极致优化。
*   **YOLO (2016)**：**You Only Look Once** 的缩写，是目标检测领域的革命性框架。它将检测任务视为一个回归问题，实现了端到端的训练和极快的推理速度，催生了一系列实时检测应用。其核心思想是使用CNN骨干网络进行特征提取，再通过检测头直接预测边界框和类别概率。

---

### 1.5 CNN的工作流程总结

1.  **输入**：原始图像。
2.  **特征提取（前向传播）**：
    *   图像经过多个 **“卷积 -> 激活 -> 池化”** 的模块。
    *   底层网络学习到**低级特征**（边缘、颜色）。
    *   中层网络学习到**中级特征**（纹理、部分器官）。
    *   高层网络学习到**高级特征**（整体物体，如“猫 脸”、“车轮”）。
3.  **分类**：高级特征被送入**全连接层**，最终通过**Softmax**输出每个类别的概率。
4.  **训练（反向传播）**：
    *   计算预测结果与真实标签的**损失**。
    *   使用**优化器（如SGD, Adam）**，通过**反向传播**算法，将损失误差从后向前传递，并更新网络中所有滤波器和权重参数。
    *   重复此过程，直到模型收敛。

---

### 1.6 CNN的应用领域（远超图像）

*   **计算机视觉**：图像分类、目标检测、图像分割、人脸识别。
*   **自然语言处理**：文本分类、情感分析、机器翻译（通过一维卷积处理序列）。
*   **游戏与机器人**：AlphaGo下围棋，机器人导航。
*   **医疗**：医学影像分析（CT、MRI片子诊断）。

---

## 二、Lenet5卷积神经网络

### 2.1 背景介绍

LeNet-5 是由 Yann LeCun 等人在1998年提出的卷积神经网络（CNN）模型，最初用于手写数字识别（MNIST 数据集）。它是现代深度学习中经典的卷积神经网络雏形，对后续的CNN发展有重要影响。

---

### 2.2 网络架构

LeNet-5 主要由以下几部分组成：

| 层名称 | 类型   | 输入尺寸         | 输出尺寸         | 参数说明              | 参数数量（含偏置） |
| --- | ---- | ------------ | ------------ | ----------------- | --------- |
| 输入层 | —    | 1 × 32 × 32  | 1 × 32 × 32  | 灰度图像输入            | 0         |
| C1  | 卷积层  | 1 × 32 × 32  | 6 × 28 × 28  | 6个卷积核，大小5×5，步长1   | 156       |
| S2  | 池化层  | 6 × 28 × 28  | 6 × 14 × 14  | 平均池化，窗口2×2，步长2    | 0         |
| C3  | 卷积层  | 6 × 14 × 14  | 16 × 10 × 10 | 16个卷积核，大小5×5，部分连接 | 2,416     |
| S4  | 池化层  | 16 × 10 × 10 | 16 × 5 × 5   | 平均池化，窗口2×2，步长2    | 0         |
| C5  | 卷积层  | 16 × 5 × 5   | 120 × 1 × 1  | 120个卷积核，大小5×5，全连接 | 48,120    |
| F6  | 全连接层 | 120          | 84           | 全连接层              | 10,164    |
| 输出层 | 全连接层 | 84           | 10           | 对应10类数字的输出        | 850       |

---

### 2.3 各层功能及原理详解

#### 1. 输入层

* 输入为 32×32 像素的灰度图像（单通道），
* 原始MNIST图像为28×28，LeNet-5采用了32×32的输入尺寸，方便边缘处理。

#### 2. C1卷积层

* 使用6个5×5卷积核，对输入进行卷积操作，步长为1，
* 输出6个28×28的特征图（通道），
* 参数总数计算：

$$
5 \times 5 \times 1 \times 6 + 6 = 150 + 6 = 156
$$

* 功能：提取图像的局部特征，如边缘、纹理。

#### 3. S2池化层（子采样层）

* 对每个通道做平均池化，窗口大小2×2，步长2，

* 输出特征图尺寸减半，变为6×14×14，

* 无需学习参数。

* 功能：降低特征图尺寸，减少计算量和过拟合风险，同时增强平移不变性。

#### 4. C3卷积层（部分连接卷积层）

* 有16个5×5卷积核，输入通道为6，
* 采用“部分连接”，不是每个输出通道都连接所有输入通道，减少参数量和计算，
* 输出16个10×10特征图。
* 参数总数计算：

$$
5 \times 5 \times 6 \times 16 + 16 = 2400 + 16 = 2,416
$$

* 功能：提取更复杂的组合特征，部分连接提升表达能力同时控制参数规模。

#### 5. S4池化层

* 结构与S2相同，平均池化，窗口2×2，步长2，

* 输出16×5×5特征图。

* 作用同S2，进一步降低尺寸和参数。

#### 6. C5卷积层（全连接卷积层）

* 卷积核大小为5×5，输入16个通道，
* 因为输入特征图为5×5，卷积后输出为1×1，
* 实际上等价于全连接层，输出120个神经元。
* 参数总数计算：

$$
5 \times 5 \times 16 \times 120 + 120 = 48,000 + 120 = 48,120
$$

* 功能：综合所有特征，形成高层次表达。

#### 7. F6全连接层

* 输入120个神经元，输出84个神经元，
* 参数总数计算：

$$
120 \times 84 + 84 = 10,080 + 84 = 10,164
$$

* 作用是进一步整合高层特征。

#### 8. 输出层

* 84维输入，10维输出，
* 对应10个数字类别的得分（logits），
* 参数总数计算：

$$
84 \times 10 + 10 = 840 + 10 = 850
$$

---

### 2.4 LeNet-5的核心原理

* **局部感受野**：卷积核只看输入局部区域，提取局部特征。
* **权值共享**：同一个卷积核在整个输入空间滑动，减少参数。
* **下采样（池化）**：降低特征图尺寸，提升平移不变性，减少计算量。
* **层级特征抽取**：从简单边缘到复杂结构的逐层特征学习。
* **部分连接**：C3层部分连接设计，平衡表达能力和参数规模。
* **端到端训练**：通过反向传播联合优化所有参数。

---

### 2.5 LeNet-5的影响和局限

#### 影响

* 是最早成功应用于图像识别的CNN架构之一，
* 为后续更深层、更复杂的网络（如AlexNet、VGG、ResNet）奠定基础。

#### 局限

* 网络较浅，容量有限，难以处理复杂大规模数据集，
* 只适合灰度小图像，未用批量归一化、激活函数优化等现代技术。

---

### 2.6 总结

| 优点         | 局限               |
| ---------- | ---------------- |
| 参数少，训练快    | 深度和宽度有限          |
| 结构简单，易理解   | 表达能力有限           |
| 权值共享、局部感受野 | 未使用现代优化技术（如ReLU） |

---

### 2.7 LeNet-5 总参数量

所有可训练参数加起来约为：

$$
156 + 0 + 2,416 + 0 + 48,120 + 10,164 + 850 = 61,706
$$

**总参数量约 61,706 个。**

---

### 2.8 参考资料

* Yann LeCun, et al., “Gradient-Based Learning Applied to Document Recognition,” Proceedings of the IEEE, 1998.
* PyTorch官方文档及LeNet-5代码实现示例。

---

## 三、Yolo

---

## 四、MobileNet