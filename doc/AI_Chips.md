# AI Chips

# AI计算体系
## 二、矩阵运算（MM）
### 2.1 卷积（Conv）简化为矩阵乘（MM） 

### 2.2 矩阵乘分块（Tiling）

### 2.3 现有应用
CPU：OpenBLAS, Intel MKI
GPU: cuBLAS, cuDNN

### 2.4 减少指令开销
* 每个指令执行更多的MACs计算
  - SIMD
  - SIMT

* 在不增加内存带宽的前提下，单时钟周期内执行更多的MACs

### 2.5 怎么做？
#### 1.软件层面
- 减少没必要的MACs
- 增加PE利用率

#### 2.硬件层面
- 减少每次MAC计算时间
  - 增加PE单元计算能力 
- 增加MACs并行计算能力
  - 增加片中PE数量
  - 支持低bits数PE计算
- 增加PE利用率
  - 增加片内Cache
  - 额外的内存带宽  

## 三、比特位宽（bitwith）
### 3.1 定义和概念
- FP32
- FP16
- TF32
- BP16
- Int32
- Int16
- Int8

S+E+M

### 3.2 降低位宽的好处

### 3.3 对AI芯片设计的思考
#### 精度的影响
#### 训练和推理的数据位宽
#### 硬件成本的开销

---

## 四、AI计算模式对硬件的诉求与思考
### 4.1 支持神经网络模型的计算逻辑
### 4.2 支持高维张量的存储与计算
### 4.3 支持常用神经网络模型结构
### 4.4 提供不同bit位数
### 4.5 利用硬件提供稀疏计算 
### 4.6 轻量化网络模型
### 4.7 大模型分布式并行

---

# AI芯片
## 一、常见处理器总概
### 1.1 CPU
### 1.2 GPU
### 1.3 NPU
### 1.4 TPU
### 1.5 超异构计算

---

## 二、并行处理架构
### 2.1 SISD
### 2.2 SIMD
### 2.3 MISD
### 2.4 MIMD
#### 共享内存MIMD
#### 分布式内存MIMD
### 2.5 SIMT

---

## 三、AI芯片指标

### 3.1 AI 计算中的三大核心概念：MAC、FLOP、OP
* OPs（Operations）
* MACs（Multiply-ACcumulate operations）
* FLOPs（Floating Point Operations）

---

#### 1. OP（Operation）——任意操作

包括：

* 整数运算（INT8）
* 浮点运算（FP16/FP32）
* bitwise、比较、ReLU 等

#### 2. OPs（操作总数量）——模型规模

* 和 FLOPs/MACs 一样属于“计算量”
* 各厂商定义不统一，但用途类似

---

#### 3. MAC（Multiply–Accumulate）—— 乘加一次

* 1 个 MAC = 做一次 $( a \times b + c )$
* 卷积层、FC 层主要就是大量 MAC。

#### 4. MACs（乘加总数量）——模型计算量

* 模型执行完整推理需要多少次 MAC。
* 不含时间信息，只是“计算量大小”。
* 例如：ResNet-50 ≈ 4e9 MACs。

---

#### 5. FLOP（Floating-point Operation）——一次浮点运算

包括加、乘、除、MAC（有时算 2 FLOPs）。

#### 6. FLOPs（浮点运算数量）——模型复杂度

* 也表示“模型计算量”
* 模型运行需要执行多少次浮点运算

⚠️ FLOPs（复数） ≠ FLOPS（per second）

---
### 3.2 算力单位:

> 带 “/s”的才是速率，即指标名字以大写“S”结尾

---

#### 1. FLOPS / GFLOPS / TFLOPS

> Floating-Point Operations **Per Second**

* 衡量 GPU / CPU 的**浮点计算能力**
* FP16、FP32 都有各自的 FLOPS

例子：
A100 FP32 = 19.5 TFLOPS
说明：每秒能做 19.5×10¹² 次浮点运算。

---

#### 2. OPS / GOPS / TOPS

> Operations **Per Second**

* 计算所有操作（整数、逻辑、bitwise）
* 常用于 NPU / TPU / FPGA 营销指标
* INT8 TOPS 常见于推理加速器

例子：
某 NPU：128 TOPS INT8
→ 每秒最多执行 128×10¹² 次 INT8 级别操作。

---

#### 3. MAC/s（MACS）/ TMAC/s

> Multiply-ACcumulate **Per Second**

* 每秒能执行多少 MAC 操作

* 很多 AI 加速器（TPU、NPU）喜欢用 MAC/s，因为卷积、矩阵乘运算基本都是 MAC

例：
某 NPU：128 TMAC/s

---

#### 数量 vs 速率

| 名称                        | 数量 | 速率（每秒） | 含义        |
| ------------------------- | -- | ------ | --------- |
| MAC                       | ✔  | ❌      | 单次乘加      |
| MACs                      | ✔  | ❌      | 模型乘加总量    |
| FLOP                      | ✔  | ❌      | 单次浮点运算    |
| FLOPs（复数）                 | ✔  | ❌      | 模型浮点量     |
| **FLOPS（/s）**             | ❌  | ✔      | GPU 浮点算力  |
| OP                        | ✔  | ❌      | 单次操作      |
| OPs（数量）                   | ✔  | ❌      | 模型总操作量    |
| **OPS / GOPS / TOPS（/s）** | ❌  | ✔      | AI 芯片运算速率 |

> 数量：**模型的计算量**
> 
> 速率：**芯片的计算能力**

---

#### 前缀对照表
| 前缀    | 含义   | 次方   | 数值                        |
| ----- | ---- | ---- | ------------------------- |
| **K** | kilo | 10³  | 1,000                     |
| **M** | Mega | 10⁶  | 1,000,000                 |
| **G** | Giga | 10⁹  | 1,000,000,000             |
| **T** | Tera | 10¹² | 1,000,000,000,000         |
| **P** | Peta | 10¹⁵ | 1,000,000,000,000,000     |
| **E** | Exa  | 10¹⁸ | 1,000,000,000,000,000,000 |

---

#### 计算关系图

```
模型计算量（与硬件无关）
┌───────────────┐
│ MACs / FLOPs / OPs │  ← 模型本身需要做多少计算
└───────────────┘
            │
            ▼
硬件算力（与模型无关）
┌────────────────────────────────┐
│ TFLOPS（浮点算力）   TOPS（整数/逻辑算力） │
└────────────────────────────────┘
```

模型大（MACs 多）→ 不一定慢
芯片算力高（TOPS 多） → 不一定快

> 真实速度还受：
> 
> * 内存带宽
> * 数据复用率
> * 并行度
> * pipeline 利用率
> * tile/block size
> * 访存模式
> 
> 影响

---

### 3.3 关键指标（Key Metrics）
#### 1. 精度（Accuracy）
##### 是什么？
* 计算精度（FP32/FP16 etc）
* 模型结果精度
  
##### 怎么提升?
* 处理各类型的无规则数据——异构平台
* 能够应对复杂的网络模型结构——计算冗余性

---

#### 2. 吞吐量（Throughput）
#### 是什么？
* 高位张量处理
* 实时性能

####  怎么提升
* PE平均利用率——负载均衡
* SOTA网络模型的运行时间——MLPerf

---

#### 3. 时延（Latency）
#### 是什么？
* 交互应用程序（TTA）

#### 怎么提升
* 通信时延对MACs的影响——优化带宽
* Batch Size大小与内存大小——多级缓存设计

---

#### 4. 能耗（Energy）
##### 是什么？
* IOT设备有限的电池容量
* 数据中心液冷能耗

##### 怎么提升
* 执行SOTA网络模型的时候Ops/W——部署场景
* 内存读写功耗——降低功耗

---

#### 5. 系统价格（System Cost）
##### 是什么？
* 硬件自身价格
* 系统集成上下游全栈等成本

##### 怎么提升
* 片内多级缓存Cache大小——内存设计
* PE数量、芯片大小、纳米制程——电路设计

---

#### 6. 易用性（Flexbility）
##### 是什么？
* 软件，SDK，工具链等配套设施

##### 怎么提升
* 对主流AI框架的支持度（Pytorch\Tensorflow）——软件栈

---

### 3.4 加速要点
#### 1. 升吞吐量 Increase Throughput


#### 2. 降低延时 Reduce Latency


#### 3. the tradeoff between Low Latency and Batch size 

#### 4. MACs
* 去掉没意义的MACs
* 降低每次MAC的计算时间

#### 5. PE阵列
* 增加PE的核心数量
* 增加PE的utilization

---

## 四、指令流水线（Instruction Pipeline）



## 四、GPU（Nvidia）




---

## 五、TPU（Google）