# AI Chips

# AI计算体系
## 二、矩阵运算（MM）
### 2.1 卷积（Conv）简化为矩阵乘（MM） 

### 2.2 矩阵乘分块（Tiling）

### 2.3 现有应用
CPU：OpenBLAS, Intel MKI
GPU: cuBLAS, cuDNN

### 2.4 减少指令开销
* 每个指令执行更多的MACs计算
  - SIMD
  - SIMT

* 在不增加内存带宽的前提下，单时钟周期内执行更多的MACs

### 2.5 怎么做？
#### 1.软件层面
- 减少没必要的MACs
- 增加PE利用率

#### 2.硬件层面
- 减少每次MAC计算时间
  - 增加PE单元计算能力 
- 增加MACs并行计算能力
  - 增加片中PE数量
  - 支持低bits数PE计算
- 增加PE利用率
  - 增加片内Cache
  - 额外的内存带宽  

---

## 三、比特位宽（bitwith）

### 3.1 定义和概念

#### 1. 浮点数的组成 — S（符号位）、E（指数位）、M（尾数位）

浮点数是科学计数法在二进制中的表示，通常分成三个部分：

| 组成    | 含义                   | 作用                    |
| ----- | -------------------- | --------------------- |
| **S** | 符号位（Sign bit）        | 表示数值的正负，0表示正数，1表示负数   |
| **E** | 指数位（Exponent）        | 用于表示数值的规模大小（放大或缩小）    |
| **M** | 尾数位（Mantissa，或称有效数字） | 表示具体数值的有效数字部分，决定数值的精度 |

---

#### 2. 浮点数表达形式

浮点数的数值计算公式通常表示为：

$$
\text{Value} = (-1)^S \times (1 + M) \times 2^{(E - \text{Bias})}
$$

* **S**：符号位，决定正负。
* **M**：尾数（小数部分），通常隐含最高位是1（即“隐藏位”），存储部分是小数部分。
* **E**：指数值，需要减去一个偏置值$（Bias）$来获得实际指数。

---
#### 3. 常见数据格式介绍

* **FP32**（Single-precision floating point，单精度浮点数）

  * 32位表示：1位符号（S）+ 8位指数（E）+ 23位尾数（M）
  * 训练主流精度，精度高，范围广。

* **FP16**（Half-precision floating point，半精度浮点数）

  * 16位表示：1位符号 + 5位指数 + 10位尾数
  * 用于训练加速和推理，计算资源和存储需求减半。

* **TF32**（Tensor Float 32）

  * NVIDIA提出的混合精度格式，基于FP32指数位数和FP16尾数位数，适合训练加速。
  * 兼顾精度和计算性能。

* **BF16**（Brain Floating Point 16）

  * 16位：1位符号 + 8位指数 + 7位尾数
  * 保留FP32的指数范围，降低尾数精度，适合训练。

* **Int32**（32位整数）

  * 用于高精度定点计算和累加操作。

* **Int16**（16位整数）

  * 常用于部分神经网络加速，兼顾精度和性能。

* **Int8**（8位整数）

  * 最常见的低精度推理格式，大幅降低计算量和存储需求。
  * 需要量化技术支持。

---

#### 4. 总结表

| 类型    | 总位数 | 符号位 (S) | 指数位 (E) | 尾数位 (M) | 指数偏置 (Bias) | 备注                       |
| ----- | --- | ------- | ------- | ------- | ----------- | ------------------------ |
| FP32  | 32  | 1       | 8       | 23      | 127         | 单精度浮点数                   |
| FP16  | 16  | 1       | 5       | 10      | 15          | 半精度浮点数                   |
| TF32  | 19  | 1       | 8       | 10      | 127         | NVIDIA提出，类似FP32指数+FP16尾数 |
| BF16  | 16  | 1       | 8       | 7       | 127         | 谷歌提出，训练友好格式              |
| Int32 | 32  | 1(最高位)     | 0       | 0       | -      | 32位带符号整数                 |
| Int16 | 16  | 1(最高位)     | 0       | 0       | -           | 16位带符号整数                 |
| Int8  | 8   | 1(最高位)     | 0       | 0       | -           | 8位带符号整数                  |

---

### 3.2 降低位宽的好处

* **减少存储需求**

  * 低位宽数据占用更少存储空间，减轻内存压力。

* **提高计算效率**

  * 低位宽算术运算硬件简单、速度快、功耗低。
  * 允许更多计算单元并行工作。

* **节省带宽**

  * 存储器与计算单元之间的数据传输减少，缓解带宽瓶颈。

* **降低功耗**

  * 位宽越小，电路切换能耗越低，尤其适合边缘设备。

---

### 3.3 对AI芯片设计的思考

#### 1. 精度的影响

* 降低位宽会带来数值表示精度下降，可能影响模型准确率。
* 浮点格式能更好地表示极大或极小的数，适合训练。
* 整数格式量化后需要精心设计和调优，防止精度大幅下降。

#### 2. 训练和推理的数据位宽

* 训练阶段一般使用FP32、FP16、BF16或TF32以保证精度和稳定性。
* 推理阶段常用INT8量化，部分应用使用INT4或更低位宽以进一步节省资源。
* 近年来，混合精度训练（如FP16 + FP32）成为主流，兼顾性能和精度。

#### 3. 硬件成本的开销

* 高位宽算术单元体积大，功耗高，设计复杂。
* 低位宽设计允许更多的算力集成在芯片面积内，提高性能密度。
* 支持多种位宽的数据通路和运算单元增加设计复杂度，但提升灵活性。
* FPGA设计中，低位宽运算减少逻辑资源占用，有利于性能提升和资源利用。

---

## 四、AI计算模式对硬件的诉求与思考
### 4.1 支持神经网络模型的计算逻辑
### 4.2 支持高维张量的存储与计算
### 4.3 支持常用神经网络模型结构
### 4.4 提供不同bit位数
### 4.5 利用硬件提供稀疏计算 
### 4.6 轻量化网络模型
### 4.7 大模型分布式并行

---

# AI芯片
## 一、常见处理器总概
### 1.1 CPU

#### 1. 发展史

#### 2. 架构

#### 3. 指令流水线（Instruction Pipeline）

### 1.2 GPU

#### 1. 发展史

#### 2. 架构

### 1.3 DSA

### 1.4 NPU

### 1.5 TPU

### 1.6 超异构计算

---

## 二、并行处理架构
### 2.1 SISD
### 2.2 SIMD
### 2.3 MISD
### 2.4 MIMD
#### 共享内存MIMD
#### 分布式内存MIMD
### 2.5 SIMT

---

## 三、AI芯片指标

### 3.1 AI 计算中的三大核心概念：MAC、FLOP、OP
* OPs（Operations）
* MACs（Multiply-ACcumulate operations）
* FLOPs（Floating Point Operations）

---

#### 1. OP（Operation）——任意操作

包括：

* 整数运算（INT8）
* 浮点运算（FP16/FP32）
* bitwise、比较、ReLU 等

#### 2. OPs（操作总数量）——模型规模

* 和 FLOPs/MACs 一样属于“计算量”
* 各厂商定义不统一，但用途类似

---

#### 3. MAC（Multiply–Accumulate）—— 乘加一次

* 1 个 MAC = 做一次 $( a \times b + c )$
* 卷积层、FC 层主要就是大量 MAC。

#### 4. MACs（乘加总数量）——模型计算量

* 模型执行完整推理需要多少次 MAC。
* 不含时间信息，只是“计算量大小”。
* 例如：ResNet-50 ≈ 4e9 MACs。

---

#### 5. FLOP（Floating-point Operation）——一次浮点运算

包括加、乘、除、MAC（有时算 2 FLOPs）。

#### 6. FLOPs（浮点运算数量）——模型复杂度

* 也表示“模型计算量”
* 模型运行需要执行多少次浮点运算

⚠️ FLOPs（复数） ≠ FLOPS（per second）

---
### 3.2 算力单位:

> 带 “/s”的才是速率，即指标名字以大写“S”结尾

---

#### 1. FLOPS / GFLOPS / TFLOPS

> Floating-Point Operations **Per Second**

* 衡量 GPU / CPU 的**浮点计算能力**
* FP16、FP32 都有各自的 FLOPS

例子：
A100 FP32 = 19.5 TFLOPS
说明：每秒能做 19.5×10¹² 次浮点运算。

---

#### 2. OPS / GOPS / TOPS

> Operations **Per Second**

* 计算所有操作（整数、逻辑、bitwise）
* 常用于 NPU / TPU / FPGA 营销指标
* INT8 TOPS 常见于推理加速器

例子：
某 NPU：128 TOPS INT8
→ 每秒最多执行 128×10¹² 次 INT8 级别操作。

---

#### 3. MAC/s（MACS）/ TMAC/s

> Multiply-ACcumulate **Per Second**

* 每秒能执行多少 MAC 操作

* 很多 AI 加速器（TPU、NPU）喜欢用 MAC/s，因为卷积、矩阵乘运算基本都是 MAC

例：
某 NPU：128 TMAC/s

---

#### 数量 vs 速率

| 名称                        | 数量 | 速率（每秒） | 含义        |
| ------------------------- | -- | ------ | --------- |
| MAC                       | ✔  | ❌      | 单次乘加      |
| MACs                      | ✔  | ❌      | 模型乘加总量    |
| FLOP                      | ✔  | ❌      | 单次浮点运算    |
| FLOPs（复数）                 | ✔  | ❌      | 模型浮点量     |
| **FLOPS（/s）**             | ❌  | ✔      | GPU 浮点算力  |
| OP                        | ✔  | ❌      | 单次操作      |
| OPs（数量）                   | ✔  | ❌      | 模型总操作量    |
| **OPS / GOPS / TOPS（/s）** | ❌  | ✔      | AI 芯片运算速率 |

> 数量：**模型的计算量**
> 
> 速率：**芯片的计算能力**

---

#### 前缀对照表
| 前缀    | 含义   | 次方   | 数值                        |
| ----- | ---- | ---- | ------------------------- |
| **K** | kilo | 10³  | 1,000                     |
| **M** | Mega | 10⁶  | 1,000,000                 |
| **G** | Giga | 10⁹  | 1,000,000,000             |
| **T** | Tera | 10¹² | 1,000,000,000,000         |
| **P** | Peta | 10¹⁵ | 1,000,000,000,000,000     |
| **E** | Exa  | 10¹⁸ | 1,000,000,000,000,000,000 |

---

#### 计算关系图

```
模型计算量（与硬件无关）
┌───────────────┐
│ MACs / FLOPs / OPs │  ← 模型本身需要做多少计算
└───────────────┘
            │
            ▼
硬件算力（与模型无关）
┌────────────────────────────────┐
│ TFLOPS（浮点算力）   TOPS（整数/逻辑算力） │
└────────────────────────────────┘
```

模型大（MACs 多）→ 不一定慢
芯片算力高（TOPS 多） → 不一定快

> 真实速度还受：
> 
> * 内存带宽
> * 数据复用率
> * 并行度
> * pipeline 利用率
> * tile/block size
> * 访存模式
> 
> 影响

---

### 3.3 关键指标（Key Metrics）
#### 1. 精度（Accuracy）
##### 是什么？
* 计算精度（FP32/FP16 etc）
* 模型结果精度
  
##### 怎么提升?
* 处理各类型的无规则数据——异构平台
* 能够应对复杂的网络模型结构——计算冗余性

---

#### 2. 吞吐量（Throughput）
#### 是什么？
* 高位张量处理
* 实时性能

####  怎么提升
* PE平均利用率——负载均衡
* SOTA网络模型的运行时间——MLPerf

---

#### 3. 时延（Latency）
#### 是什么？
* 交互应用程序（TTA）

#### 怎么提升
* 通信时延对MACs的影响——优化带宽
* Batch Size大小与内存大小——多级缓存设计

---

#### 4. 能耗（Energy）
##### 是什么？
* IOT设备有限的电池容量
* 数据中心液冷能耗

##### 怎么提升
* 执行SOTA网络模型的时候Ops/W——部署场景
* 内存读写功耗——降低功耗

---

#### 5. 系统价格（System Cost）
##### 是什么？
* 硬件自身价格
* 系统集成上下游全栈等成本

##### 怎么提升
* 片内多级缓存Cache大小——内存设计
* PE数量、芯片大小、纳米制程——电路设计

---

#### 6. 易用性（Flexbility）
##### 是什么？
* 软件，SDK，工具链等配套设施

##### 怎么提升
* 对主流AI框架的支持度（Pytorch\Tensorflow）——软件栈

---

### 3.4 加速要点
#### 1. 升吞吐量 Increase Throughput


#### 2. 降低延时 Reduce Latency


#### 3. the tradeoff between Low Latency and Batch size 

#### 4. MACs
* 去掉没意义的MACs
* 降低每次MAC的计算时间

#### 5. PE阵列
* 增加PE的核心数量
* 增加PE的utilization

---

## 四、AI芯片任务与部署

### 4.1 任务

#### 1. 训练

#### 2. 推理

---

### 4.2 部署

#### 1. 云端

#### 2. 边缘端侧

---

### 五、AI芯片的技术路线

#### 5.1 总概
1. GPU —— 通用可编程并行处理（训练主力）
2. NPU —— 专用矩阵阵列（推理能效最高）
3. DSP —— 移动端轻量 AI 加速
4. FPGA —— 可重配置加速器（原型、边缘、特殊场景）
5. ASIC —— 极致能效（超定制矩阵阵列）
6. DSA —— 领域专用架构（GNN、推荐、LLM 推理）

#### 5.2 对比表
| 路线       | 通用性  | 训练    | 推理能效  | 软件生态  | 灵活性   | 成本 | 典型应用          |
| -------- | ---- | ----- | ----- | ----- | ----- | -- | ------------- |
| **GPU**  | ⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐   | ⭐⭐⭐⭐⭐ | ⭐⭐⭐   | 高  | 训练、通用推理       |
| **NPU**  | ⭐⭐   | ⭐     | ⭐⭐⭐⭐⭐ | ⭐⭐⭐   | ⭐⭐    | 中  | 数据中心推理        |
| **DSP**  | ⭐⭐   | ⭐     | ⭐⭐⭐⭐  | ⭐⭐    | ⭐⭐⭐   | 低  | 手机端 AI        |
| **FPGA** | ⭐⭐⭐  | ⭐     | ⭐⭐    | ⭐     | ⭐⭐⭐⭐⭐ | 高  | 原型、低延迟应用      |
| **ASIC** | ⭐    | ⭐     | ⭐⭐⭐⭐⭐ | ⭐     | ⭐     | 高  | 特定推理、大规模部署    |
| **DSA**  | ⭐    | ⭐     | ⭐⭐⭐⭐⭐ | ⭐     | ⭐     | 高  | GNN、推荐、LLM 推理 |


---

## 五、GPU（Nvidia）




---

## 六、TPU（Google）